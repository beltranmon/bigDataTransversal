{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de BERT-for-Sentiment-Analysis.ipynb","provenance":[{"file_id":"1f32gj5IYIyFipoINiC8P3DvKat-WWLUK","timestamp":1619708721259}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d0378a87faa34212ae7fff4776c603fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4e8146d9b21c48e893c7d29be9720c38","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4ca23b7999e149299df32f019e2fd1d4","IPY_MODEL_34fdbeb65f8143c5951e93dd4be06e4b"]}},"4e8146d9b21c48e893c7d29be9720c38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4ca23b7999e149299df32f019e2fd1d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8c51bd114179480ab57cbed42734161c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2915ecf2c18b4210a715495218412f75"}},"34fdbeb65f8143c5951e93dd4be06e4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c44e5db6dc834557af0c6289e23f5160","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 2.83MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60967f90c62a4c4a83badf77469bcfc6"}},"8c51bd114179480ab57cbed42734161c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2915ecf2c18b4210a715495218412f75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c44e5db6dc834557af0c6289e23f5160":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"60967f90c62a4c4a83badf77469bcfc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b7dd8c1b7c64f17b5b04012a953ab14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bf444a0ee18f4f1b82d24cae1107eef5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_48a4b3ab7ead4e8cb8a6a34f648da1f5","IPY_MODEL_d96048c7d6f94d3a81fd315730b22e29"]}},"bf444a0ee18f4f1b82d24cae1107eef5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"48a4b3ab7ead4e8cb8a6a34f648da1f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ed5004915afe4cad95c074d1fdb37045","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a1b88ae983a74cd3a70baab4d2c628e3"}},"d96048c7d6f94d3a81fd315730b22e29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c423384aaae64df8b6970863e21cec9e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 2.21kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2bdd19b41e2d4746baf29d2e2aa09d35"}},"ed5004915afe4cad95c074d1fdb37045":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a1b88ae983a74cd3a70baab4d2c628e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c423384aaae64df8b6970863e21cec9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2bdd19b41e2d4746baf29d2e2aa09d35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6cb9f0a1b20d4ebcb400f084b1c467f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_abc1a7b926964c818d4918f0313085db","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9216bd11632e4aeb8c8eeba00c43ca5f","IPY_MODEL_3174dba24eef45e2b317711a49949573"]}},"abc1a7b926964c818d4918f0313085db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9216bd11632e4aeb8c8eeba00c43ca5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e57eb9ae375a414f90b381565f0ee628","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5cbebc20df6b415b8cbd7a43ca20db9b"}},"3174dba24eef45e2b317711a49949573":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_419118c1a0ff45c785c44501b9b7aabc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:21&lt;00:00, 20.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3e04193af4b84422836acb05efbc4b30"}},"e57eb9ae375a414f90b381565f0ee628":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5cbebc20df6b415b8cbd7a43ca20db9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"419118c1a0ff45c785c44501b9b7aabc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3e04193af4b84422836acb05efbc4b30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"sqg4y371CM7X"},"source":["# Fine-tuning BERT for Sentiment Analysis\n"]},{"cell_type":"markdown","metadata":{"id":"M2D_810Uruqe"},"source":["# A - Introduction"]},{"cell_type":"markdown","metadata":{"id":"IT93d88Pcepn"},"source":["In recent years the NLP community has seen many breakthoughs in Natural Language Processing, especially the shift to transfer learning. Models like ELMo, fast.ai's ULMFiT, Transformer and OpenAI's GPT have allowed researchers to achieves state-of-the-art results on multiple benchmarks and provided the community with large pre-trained models with high performance. This shift in NLP is seen as NLP's ImageNet moment, a shift in computer vision a few year ago when lower layers of deep learning networks with million of parameters trained on a specific task can be reused and fine-tuned for other tasks, rather than training new networks from scratch.\n","\n","One of the most biggest milestones in the evolution of NLP recently is the release of Google's BERT, which is described as the beginning of a new era in NLP. In this notebook I'll use the HuggingFace's `transformers` library to fine-tune pretrained BERT model for a classification task.\n","\n","**Reference**:\n","\n","To understand **Transformer** (the architecture which BERT is built on) and learn how to implement BERT, I highly recommend reading the following sources:\n","\n","- [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/): A very clear and well-written guide to understand BERT.\n","- [The documentation of the `transformers` library](https://huggingface.co/transformers/v2.2.0/index.html)\n","- [BERT Fine-Tuning Tutorial with PyTorch](http://mccormickml.com/2019/07/22/BERT-fine-tuning/) by [Chris McCormick](http://mccormickml.com/): A very detailed tutorial showing how to use BERT with the HuggingFace PyTorch library.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"slO_rmYgwmmE"},"source":["# B - Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YaH9XqPPMBYn","executionInfo":{"status":"ok","timestamp":1619790181985,"user_tz":-120,"elapsed":19854,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"08b4fca5-8f20-4e12-f36c-0b5044eaefa2"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCds4dteMnG_","executionInfo":{"status":"ok","timestamp":1619790182992,"user_tz":-120,"elapsed":20851,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"21c3412a-a1b5-4d74-b143-7d028da62f26"},"source":["%cd drive/MyDrive/PATH_TO_FOLDER"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/noEstructurado/TEXT\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"31OW0dhozvli"},"source":["## 1. Load Essential Libraries"]},{"cell_type":"code","metadata":{"id":"_lTXsMK3sNYr","executionInfo":{"status":"ok","timestamp":1619790186897,"user_tz":-120,"elapsed":24754,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["import os\n","import re\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","#Load the libraries\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import nltk\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelBinarizer\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from wordcloud import WordCloud,STOPWORDS\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize,sent_tokenize\n","from bs4 import BeautifulSoup\n","import spacy\n","import re,string,unicodedata\n","from nltk.tokenize.toktok import ToktokTokenizer\n","from nltk.stem import LancasterStemmer,WordNetLemmatizer\n","from sklearn.linear_model import LogisticRegression,SGDClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from textblob import TextBlob\n","from textblob import Word\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","\n","%matplotlib inline"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u07WRKnxsX96"},"source":["## 2. Dataset"]},{"cell_type":"markdown","metadata":{"id":"j1_Tpie3tGp3"},"source":["### 2.1. Download Dataset"]},{"cell_type":"code","metadata":{"_uuid":"4c593c17588723c0b0b0f19851cb70a8447ced76","scrolled":true,"id":"79xUhcP1_y8V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619790189432,"user_tz":-120,"elapsed":27282,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"bc1183b4-2bb8-4b41-8caf-4685dca87e7a"},"source":["#importing the training data\n","imdb_data=pd.read_csv('IMDB Dataset.csv')[:5000]\n","print(imdb_data.shape)\n","imdb_data.head(10)\n","\n","imdb_data['label']= imdb_data['sentiment'].apply(lambda x: x.replace(\"positive\",\"1\").replace(\"negative\",\"0\"))\n","\n","imdb_data.pop('sentiment')\n","\n","imdb_data['id'] = imdb_data.index\n","\n","imbd_data_complete = imdb_data.copy()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(6000, 2)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"1QLCwOkiNT9o","executionInfo":{"status":"ok","timestamp":1619790189433,"user_tz":-120,"elapsed":27274,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"e9a37a75-97d8-4c09-e86a-ebc032239a23"},"source":["imdb_data.head(10)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>label</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Probably my all-time favorite movie, a story o...</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>I sure would like to see a resurrection of a u...</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>This show was an amazing, fresh &amp; innovative i...</td>\n","      <td>0</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Encouraged by the positive comments about this...</td>\n","      <td>0</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>If you like original gut wrenching laughter yo...</td>\n","      <td>1</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review label  id\n","0  One of the other reviewers has mentioned that ...     1   0\n","1  A wonderful little production. <br /><br />The...     1   1\n","2  I thought this was a wonderful way to spend ti...     1   2\n","3  Basically there's a family where a little boy ...     0   3\n","4  Petter Mattei's \"Love in the Time of Money\" is...     1   4\n","5  Probably my all-time favorite movie, a story o...     1   5\n","6  I sure would like to see a resurrection of a u...     1   6\n","7  This show was an amazing, fresh & innovative i...     0   7\n","8  Encouraged by the positive comments about this...     0   8\n","9  If you like original gut wrenching laughter yo...     1   9"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"_uuid":"90da29c3b79f46f41d7391a2a116065b616d0fac","id":"iuh9-BJf_y8X"},"source":["**Text normalization**"]},{"cell_type":"code","metadata":{"_uuid":"f000c43d91f68f6668539f089c6a54c5ce3bd819","id":"wo9pJJWe_y8X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619790189434,"user_tz":-120,"elapsed":27268,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"e7f84d39-a021-4116-e98e-57cab0e1ef3d"},"source":["nltk.download('stopwords')\n","#Tokenization of text\n","tokenizer=ToktokTokenizer()\n","#Setting English stopwords\n","stopword_list=nltk.corpus.stopwords.words('english')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_uuid":"328b6e5977da3e055ad4b2e11a31e5e12ccf3b16","id":"sTTRk30m_y8X"},"source":["**Removing html strips and noise text**"]},{"cell_type":"code","metadata":{"_uuid":"6f6fcafbdadcdcb0c164e37d71fb9d1623f74d0a","id":"BnpAsMzs_y8Y","executionInfo":{"status":"ok","timestamp":1619790190196,"user_tz":-120,"elapsed":28028,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["#Removing the html strips\n","def strip_html(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    return soup.get_text()\n","\n","#Removing the square brackets\n","def remove_between_square_brackets(text):\n","    return re.sub('\\[[^]]*\\]', '', text)\n","\n","#Removing the noisy text\n","def denoise_text(text):\n","    text = strip_html(text)\n","    text = remove_between_square_brackets(text)\n","    return text\n","#Apply function on review column\n","imdb_data['review']=imdb_data['review'].apply(denoise_text)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"88117b74761d1047924d6d70f76642faa0e706ac","id":"VW-5f5ye_y8Y"},"source":["**Removing special characters**"]},{"cell_type":"code","metadata":{"_uuid":"219da72b025121fd98081df50ae0fcaace10cc9d","id":"JW3knwp3_y8Y","executionInfo":{"status":"ok","timestamp":1619790190197,"user_tz":-120,"elapsed":28027,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["#Define function for removing special characters\n","def remove_special_characters(text, remove_digits=True):\n","    pattern=r'[^a-zA-z0-9\\s]'\n","    text=re.sub(pattern,'',text)\n","    return text\n","#Apply function on review column\n","imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3b66eeabd5b7b8c251f8b8ddf331140a64bcd514","id":"-4_iB8Tg_y8Y"},"source":["**Text stemming\n","**"]},{"cell_type":"code","metadata":{"_uuid":"2295f2946e0ab74c220ad538d0e7adc04d23f697","id":"30iXt2nH_y8Y","executionInfo":{"status":"ok","timestamp":1619790213317,"user_tz":-120,"elapsed":51146,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["#Stemming the text\n","def simple_stemmer(text):\n","    ps=nltk.porter.PorterStemmer()\n","    text= ' '.join([ps.stem(word) for word in text.split()])\n","    return text\n","#Apply function on review column\n","imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e83107e4a281d84d7ae42b4e2c8d81b7ece438e4","id":"MIHQM8oO_y8Z"},"source":["**Removing stopwords**"]},{"cell_type":"code","metadata":{"_uuid":"5dbff82b4d2d188d8777b273a75d8ac714d38885","id":"p1O7ya9Q_y8Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619790217357,"user_tz":-120,"elapsed":55179,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"045e82ba-7bb4-4e5b-fbf0-1c83be7f5c21"},"source":["#set stopwords to english\n","stop=set(stopwords.words('english'))\n","print(stop)\n","\n","#removing the stopwords\n","def remove_stopwords(text, is_lower_case=False):\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        filtered_tokens = [token for token in tokens if token not in stopword_list]\n","    else:\n","        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","    filtered_text = ' '.join(filtered_tokens)    \n","    return filtered_text\n","#Apply function on review column\n","imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["{'or', 'don', 'while', 'down', 'other', \"you've\", 'our', 'ours', \"mightn't\", 'most', 'needn', 'his', 'in', 'mustn', 'hers', 'below', 'too', 'were', 'from', 'ourselves', 'herself', 'during', 'i', 'did', 'as', 'further', 'should', 'will', 'yourself', 'was', \"you're\", 'my', 'which', 'where', 'hasn', 'to', 'ain', 'we', 'wouldn', 'once', 'under', \"haven't\", 'with', \"should've\", 'few', 'off', \"shan't\", 'had', 'am', 'an', 'does', 'themselves', 'up', 'now', 'ma', 'at', 're', 'your', 'out', 'here', 'them', 'do', 'o', 'whom', 'being', 'all', 'be', 'itself', 'isn', 'above', 'than', 'couldn', 'won', 'this', 'nor', 'the', 'doesn', 'so', 'again', 'hadn', \"it's\", 'when', 'haven', 'on', \"weren't\", 'about', 'it', 'she', 'why', 'its', 'himself', 'any', 'her', 'no', 't', \"you'll\", 'you', 'm', 'doing', 've', 'more', \"didn't\", 'll', 'against', 'for', 'through', 'there', \"shouldn't\", 'have', 'didn', 'him', \"you'd\", \"wouldn't\", 'each', 'then', 'those', \"don't\", 'what', 'has', 'a', 'after', 'before', 'yourselves', 'myself', 'y', \"wasn't\", 'me', 'yours', \"isn't\", 'mightn', 'theirs', 'own', 'over', 's', 'shan', 'how', \"hasn't\", 'having', 'he', 'but', 'd', \"couldn't\", 'by', \"hadn't\", 'are', 'just', 'same', 'shouldn', \"that'll\", 'wasn', 'of', 'can', \"mustn't\", 'and', 'into', 'not', 'very', 'weren', \"won't\", 'if', 'such', \"she's\", 'only', 'aren', 'that', \"doesn't\", 'some', 'because', 'until', 'they', \"aren't\", 'who', 'is', 'both', 'been', 'their', \"needn't\", 'these', 'between'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"Nq1iwj6JPbPN","executionInfo":{"status":"ok","timestamp":1619790217360,"user_tz":-120,"elapsed":55173,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"d316e908-8bca-4e0a-f430-3d4f67a846c1"},"source":["imdb_data.head(10)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>label</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>one review ha mention watch 1 Oz episod youll ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wonder littl product film techniqu veri unassu...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>thought thi wa wonder way spend time hot summe...</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>basic famili littl boy jake think zombi hi clo...</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>petter mattei love time money visual stun film...</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>probabl alltim favorit movi stori selfless sac...</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>sure would like see resurrect date seahunt ser...</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>thi show wa amaz fresh innov idea 70 first air...</td>\n","      <td>0</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>encourag posit comment thi film wa look forwar...</td>\n","      <td>0</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>like origin gut wrench laughter like thi movi ...</td>\n","      <td>1</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review label  id\n","0  one review ha mention watch 1 Oz episod youll ...     1   0\n","1  wonder littl product film techniqu veri unassu...     1   1\n","2  thought thi wa wonder way spend time hot summe...     1   2\n","3  basic famili littl boy jake think zombi hi clo...     0   3\n","4  petter mattei love time money visual stun film...     1   4\n","5  probabl alltim favorit movi stori selfless sac...     1   5\n","6  sure would like see resurrect date seahunt ser...     1   6\n","7  thi show wa amaz fresh innov idea 70 first air...     0   7\n","8  encourag posit comment thi film wa look forwar...     0   8\n","9  like origin gut wrench laughter like thi movi ...     1   9"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"5-kK_NX2Yehk","executionInfo":{"status":"ok","timestamp":1619790217361,"user_tz":-120,"elapsed":55172,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["imdb_test_data = imdb_data[4000:4999]\n","imdb_data = imdb_data[0:3999]"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cp-vfxKZvl6M"},"source":["We will randomly split the entire training data into two sets: a train set with 90% of the data and a validation set with 10% of the data."]},{"cell_type":"code","metadata":{"id":"X4HKAFTbvMwI","executionInfo":{"status":"ok","timestamp":1619790217361,"user_tz":-120,"elapsed":55170,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","X = data.tweet.values\n","y = data.label.values\n","\n","X_train, X_val, y_train, y_val =\\\n","    train_test_split(X, y, test_size=0.1, random_state=2020)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X79dYY3sxDCi"},"source":["## 3. Set up GPU for training"]},{"cell_type":"markdown","metadata":{"id":"Zi1CoEOL1puh"},"source":["Google Colab offers free GPUs and TPUs. Since we'll be training a large neural network it's best to utilize these features.\n","\n","A GPU can be added by going to the menu and selecting:\n","\n","`Runtime -> Change runtime type -> Hardware accelerator: GPU`\n","\n","Then we need to run the following cell to specify the GPU as the device."]},{"cell_type":"code","metadata":{"id":"K7hxtI4l0SUJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619790220871,"user_tz":-120,"elapsed":58673,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"f373e0cd-992b-46ef-bc2a-1a5e38711482"},"source":["import torch\n","\n","if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","Device name: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lEPPYHa62JXF"},"source":["# D - Fine-tuning BERT"]},{"cell_type":"markdown","metadata":{"id":"bYJRzWI73eBJ"},"source":["## 1. Install the Hugging Face Library"]},{"cell_type":"markdown","metadata":{"id":"Yxv-EJ2j31Iv"},"source":["The transformer library of Hugging Face contains PyTorch implementation of state-of-the-art NLP models including BERT (from Google), GPT (from OpenAI) ... and pre-trained model weights."]},{"cell_type":"code","metadata":{"id":"uFiv8WGl4p40","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619790229324,"user_tz":-120,"elapsed":67118,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"fb63316b-e2e7-452f-eb4c-1fd60700cff0"},"source":["!pip install transformers==2.8.0"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Collecting transformers==2.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\r\u001b[K     |▋                               | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 31.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 20.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 23.5MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 25.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 27.7MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 28.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 22.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 46.2MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/0d/63f4deac9106834127d7a760b934faa8012aa5c1ce0be94758c8f0bc75ab/boto3-1.17.61-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 27.2MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 46.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 56.0MB/s \n","\u001b[?25hCollecting botocore<1.21.0,>=1.20.61\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/34/5daba930f1807ba99bec7b3ed41d335bd406d84aceea13e2ded894a6dd0a/botocore-1.20.61-py2.py3-none-any.whl (7.5MB)\n","\u001b[K     |████████████████████████████████| 7.5MB 39.2MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer<0.5.0,>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 10.5MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.61->boto3->transformers==2.8.0) (2.8.1)\n","\u001b[31mERROR: botocore 1.20.61 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: tokenizers, jmespath, botocore, s3transfer, boto3, sentencepiece, sacremoses, transformers\n","Successfully installed boto3-1.17.61 botocore-1.20.61 jmespath-0.10.0 s3transfer-0.4.2 sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i3acv6s95YYr"},"source":["### 2.1. BERT Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"C1fRHtdU5dEn"},"source":["In order to apply the pre-trained BERT, we must use the tokenizer provided by the library. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words.\n","\n","In addition, we are required to add special tokens to the start and end of each sentence, pad & truncate all sentences to a single constant length, and explicitly specify what are padding tokens with the \"attention mask\".\n","\n","The `encode_plus` method of BERT tokenizer will:\n","\n","(1) split our text into tokens,\n","\n","(2) add the special `[CLS]` and `[SEP]` tokens, and\n","\n","(3) convert these tokens into indexes of the tokenizer vocabulary,\n","\n","(4) pad or truncate sentences to max length, and\n","\n","(5) create attention mask.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"yDAfbCle59tP","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["d0378a87faa34212ae7fff4776c603fb","4e8146d9b21c48e893c7d29be9720c38","4ca23b7999e149299df32f019e2fd1d4","34fdbeb65f8143c5951e93dd4be06e4b","8c51bd114179480ab57cbed42734161c","2915ecf2c18b4210a715495218412f75","c44e5db6dc834557af0c6289e23f5160","60967f90c62a4c4a83badf77469bcfc6"]},"executionInfo":{"status":"ok","timestamp":1619790230407,"user_tz":-120,"elapsed":68199,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"7f4a7935-8d2c-4e21-f850-7908481a013d"},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","# Create a function to tokenize a set of texts\n","def preprocessing_for_bert(data):\n","    \"\"\"Perform required preprocessing steps for pretrained BERT.\n","    @param    data (np.array): Array of texts to be processed.\n","    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n","    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n","                  tokens should be attended to by the model.\n","    \"\"\"\n","    # Create empty lists to store outputs\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in data:\n","        # `encode_plus` will:\n","        #    (1) Tokenize the sentence\n","        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n","        #    (3) Truncate/Pad sentence to max length\n","        #    (4) Map tokens to their IDs\n","        #    (5) Create attention mask\n","        #    (6) Return a dictionary of outputs\n","        encoded_sent = tokenizer.encode_plus(\n","            text=sent,  # Preprocess sentence\n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,                  # Max length to truncate/pad\n","            pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True      # Return attention mask\n","            )\n","        \n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0378a87faa34212ae7fff4776c603fb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TNE9oASMZ1bN"},"source":["Before tokenizing, we need to specify the maximum length of our sentences."]},{"cell_type":"code","metadata":{"id":"hrbvKGNAlMtt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619790240998,"user_tz":-120,"elapsed":78783,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"ee66124f-e29d-40fc-ab5f-ee63cb9fa0ad"},"source":["# Concatenate train data and test data\n","all_reviews = np.concatenate([imdb_data.review.values, imdb_test_data.review.values])\n","\n","# Encode our concatenated data\n","# BERT maximum document data is 512 characters\n","encoded_reviews = [tokenizer.encode(sent[0:511], add_special_tokens=True) for sent in all_reviews]\n","\n","# Find the maximum length\n","max_len = max([len(sent) for sent in encoded_reviews])\n","print('Max length: ', max_len)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Max length:  174\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vpdjBB9fmbu2"},"source":["Now let's tokenize our data."]},{"cell_type":"code","metadata":{"id":"QTlQzTzAfCy7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619790256315,"user_tz":-120,"elapsed":94092,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"6e426ad3-640d-45f4-e6fd-c422266077e0"},"source":["# Specify `MAX_LEN`\n","MAX_LEN = 512\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train)\n","val_inputs, val_masks = preprocessing_for_bert(X_val)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Original:  one review ha mention watch 1 Oz episod youll hook right thi exactli happen meth first thing struck Oz wa brutal unflinch scene violenc set right word GO trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call OZ nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda Em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast Oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch Oz may becom comfort uncomfort viewingthat get touch darker side\n","Token IDs:  [101, 2028, 3319, 5292, 5254, 3422, 1015, 11472, 4958, 19565, 2094, 2017, 3363, 8103, 2157, 16215, 2072, 6635, 3669, 4148, 2777, 2232, 2034, 2518, 4930, 11472, 11333, 12077, 4895, 10258, 2378, 2818, 3496, 6819, 9890, 12273, 2275, 2157, 2773, 2175, 3404, 16215, 2072, 2265, 8143, 2540, 5199, 3593, 16215, 2072, 2265, 4139, 8595, 7634, 4319, 3348, 6819, 9890, 12273, 2524, 27108, 4438, 2224, 2773, 4183, 2655, 11472, 4172, 13129, 2445, 17411, 4555, 10819, 3126, 2110, 7279, 4221, 12380, 3089, 3579, 2364, 3669, 14110, 25022, 3775, 7551, 2930, 3827, 3526, 3221, 2392, 2227, 20546, 26927, 24887, 2072, 2152, 11376, 7861, 25022, 3775, 2188, 2116, 5649, 2319, 5152, 18542, 2696, 7402, 3017, 3059, 3493, 2062, 6499, 8040, 16093, 10258, 2331, 6237, 26489, 5856, 3066, 21146, 4305, 3820, 2196, 2521, 2185, 2072, 2052, 2360, 2364, 5574, 2265, 2349, 2755, 2175, 2063, 2265, 2876, 2102, 8108, 5293, 3653, 6916, 27263, 20689, 6773, 7731, 20075, 2368, 2278, 5293, 11084, 5293, 7472, 18153, 2987, 2102, 6752, 2105, 2034, 4958, 19565, 2094, 2412, 2387, 4930, 17235, 3775, 11333, 16524, 2481, 2102, 2360, 11333, 3191, 2072, 3422, 4503, 11937, 3367, 11472, 2288, 16222, 19966, 5358, 2152, 2504, 8425, 6819, 9890, 12273, 6819, 9890, 12273, 1999, 29427, 2594, 19302, 3457, 2040, 3363, 2853, 15519, 1999, 18900, 2040, 3363, 3102, 2344, 2131, 2185, 2092, 5450, 3054, 19422, 2465, 1999, 18900, 2735, 3827, 7743, 2349, 3768, 2395, 8066, 3827, 4654, 4842, 2072, 3422, 11472, 2089, 2022, 9006, 7216, 4895, 9006, 13028, 10523, 8322, 2102, 2131, 3543, 9904, 2217, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Tokenizing data...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aZU8t5VNfvhY"},"source":["### 2.2. Create PyTorch DataLoader"]},{"cell_type":"markdown","metadata":{"id":"aoHdl3gFgMZY"},"source":["We will create an iterator for our dataset using the torch DataLoader class. This will help save on memory during training and boost the training speed."]},{"cell_type":"code","metadata":{"id":"xHuYEc61gcGL","executionInfo":{"status":"ok","timestamp":1619790256316,"user_tz":-120,"elapsed":94092,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","y_train = np.asarray(list(y_train), dtype=np.float32)\n","y_val = np.asarray(list(y_val), dtype=np.float32)\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train, dtype = torch.long)\n","val_labels = torch.tensor(y_val, dtype = torch.long)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 8\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSRAga-yj17q"},"source":["## 3. Train Our Model"]},{"cell_type":"markdown","metadata":{"id":"KoOdsDgG8b_Z"},"source":["### 3.1. Create BertClassifier"]},{"cell_type":"markdown","metadata":{"id":"zA_yESCl5nuK"},"source":["BERT-base consists of 12 transformer layers, each transformer layer takes in a list of token embeddings, and produces the same number of embeddings with the same hidden size (or dimensions) on the output. The output of the final transformer layer of the `[CLS]` token is used as the features of the sequence to feed a classifier.\n","\n","The `transformers` library has the [`BertForSequenceClassification`](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification) class which is designed for classification tasks. However, we will create a new class so we can specify our own choice of classifiers.\n","\n","Below we will create a BertClassifier class with a BERT model to extract the last hidden layer of the `[CLS]` token and a single-hidden-layer feed-forward neural network as our classifier."]},{"cell_type":"code","metadata":{"id":"YK41aBFSj5jK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619790256316,"user_tz":-120,"elapsed":94084,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"56d5093a-7af6-4bed-b8bc-f6e1fce9f0de"},"source":["%%time\n","import torch\n","import torch.nn as nn\n","from transformers import BertModel\n","import gc\n","\n","# Create the BertClassfier class\n","class BertClassifier(nn.Module):\n","    \"\"\"Bert Model for Classification Tasks.\n","    \"\"\"\n","    def __init__(self, freeze_bert=False):\n","        \"\"\"\n","        @param    bert: a BertModel object\n","        @param    classifier: a torch.nn.Module classifier\n","        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n","        \"\"\"\n","        super(BertClassifier, self).__init__()\n","        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n","        D_in, H, D_out = 768, 50, 2\n","\n","        # Instantiate BERT model\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, H),\n","            nn.ReLU(),\n","            #nn.Dropout(0.5),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","        \n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","        Feed input to BERT and the classifier to compute logits.\n","        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n","                      max_length)\n","        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n","                      information with shape (batch_size, max_length)\n","        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n","                      num_labels)\n","        \"\"\"\n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask)\n","        \n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"],"execution_count":20,"outputs":[{"output_type":"stream","text":["CPU times: user 35 µs, sys: 0 ns, total: 35 µs\n","Wall time: 38.1 µs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LwNrCgPh-yR7"},"source":["### 3.2. Optimizer & Learning Rate Scheduler"]},{"cell_type":"markdown","metadata":{"id":"V6iOXiN8-8gc"},"source":["To fine-tune our Bert Classifier, we need to create an optimizer. The authors recommend following hyper-parameters:\n","\n","- Batch size: 16 or 32\n","- Learning rate (Adam): 5e-5, 3e-5 or 2e-5\n","- Number of epochs: 2, 3, 4"]},{"cell_type":"code","metadata":{"id":"JX7su7Q_269U","executionInfo":{"status":"ok","timestamp":1619790256317,"user_tz":-120,"elapsed":94084,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","def initialize_model(epochs=4):\n","    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n","    \"\"\"\n","    # Instantiate Bert Classifier\n","    bert_classifier = BertClassifier(freeze_bert=False)\n","\n","    # Tell PyTorch to run the model on GPU\n","    bert_classifier.to(device)\n","\n","    # Create the optimizer\n","    optimizer = AdamW(bert_classifier.parameters(),\n","                      lr=5e-5,    # Default learning rate\n","                      eps=1e-8    # Default epsilon value\n","                      )\n","\n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Set up the learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0, # Default value\n","                                                num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"41DRNjv4B0Ow"},"source":["### 3.3. Training Loop"]},{"cell_type":"markdown","metadata":{"id":"VYU-GQRZG0y8"},"source":["We will train our Bert Classifier. In each epoch, we will train our model and evaluate its performance on the validation set. In more details, we will:\n","\n","Training:\n","- Unpack our data from the dataloader and load the data onto the GPU\n","- Zero out gradients calculated in the previous pass\n","- Perform a forward pass to compute logits and loss\n","- Perform a backward pass to compute gradients (`loss.backward()`)\n","- Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","- Update the model's parameters (`optimizer.step()`)\n","- Update the learning rate (`scheduler.step()`)\n","\n","Evaluation:\n","- Unpack our data and load onto the GPU\n","- Forward pass\n","- Compute loss and accuracy rate over the validation set\n","\n","The script below is commented with the details of our training and evaluation loop. "]},{"cell_type":"code","metadata":{"id":"Xy4HkhyECibW","executionInfo":{"status":"ok","timestamp":1619790256630,"user_tz":-120,"elapsed":94395,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["import random\n","import time\n","\n","# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n","    \"\"\"Train the BertClassifier model.\n","    \"\"\"\n","    \n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        # =======================================\n","        #               Training\n","        # =======================================\n","        # Print the header of the result table\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        # Measure the elapsed time of each epoch\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        # Reset tracking variables at the beginning of each epoch\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            # Load batch to GPU\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and the learning rate\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # After the completion of each training epoch, measure the model's performance\n","            # on our validation set.\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            \n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","    \n","    print(\"Training complete!\")\n","\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5DW6grRmfT-"},"source":["### 3.5. Train Our Model on the Entire Training Data"]},{"cell_type":"code","metadata":{"id":"JkMK5VqJJvSO","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1b7dd8c1b7c64f17b5b04012a953ab14","bf444a0ee18f4f1b82d24cae1107eef5","48a4b3ab7ead4e8cb8a6a34f648da1f5","d96048c7d6f94d3a81fd315730b22e29","ed5004915afe4cad95c074d1fdb37045","a1b88ae983a74cd3a70baab4d2c628e3","c423384aaae64df8b6970863e21cec9e","2bdd19b41e2d4746baf29d2e2aa09d35","6cb9f0a1b20d4ebcb400f084b1c467f9","abc1a7b926964c818d4918f0313085db","9216bd11632e4aeb8c8eeba00c43ca5f","3174dba24eef45e2b317711a49949573","e57eb9ae375a414f90b381565f0ee628","5cbebc20df6b415b8cbd7a43ca20db9b","419118c1a0ff45c785c44501b9b7aabc","3e04193af4b84422836acb05efbc4b30"]},"executionInfo":{"status":"ok","timestamp":1619791367655,"user_tz":-120,"elapsed":1205412,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"df71818b-86ef-40cb-8535-dcd50d18e4b7"},"source":["# Concatenate the train set and the validation set\n","full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n","full_train_sampler = RandomSampler(full_train_data)\n","full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=4)\n","\n","# Train the Bert Classifier on the entire training data\n","set_seed(42)\n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n","train(bert_classifier, full_train_dataloader, epochs=2)"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b7dd8c1b7c64f17b5b04012a953ab14","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6cb9f0a1b20d4ebcb400f084b1c467f9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"],"name":"stderr"},{"output_type":"stream","text":["   1    |   20    |   0.694682   |     -      |     -     |   8.93   \n","   1    |   40    |   0.698142   |     -      |     -     |   8.37   \n","   1    |   60    |   0.698233   |     -      |     -     |   8.46   \n","   1    |   80    |   0.686101   |     -      |     -     |   8.66   \n","   1    |   100   |   0.709895   |     -      |     -     |   8.83   \n","   1    |   120   |   0.696045   |     -      |     -     |   9.03   \n","   1    |   140   |   0.670547   |     -      |     -     |   9.06   \n","   1    |   160   |   0.677247   |     -      |     -     |   8.87   \n","   1    |   180   |   0.666896   |     -      |     -     |   8.77   \n","   1    |   200   |   0.682399   |     -      |     -     |   8.69   \n","   1    |   220   |   0.550851   |     -      |     -     |   8.60   \n","   1    |   240   |   0.657334   |     -      |     -     |   8.64   \n","   1    |   260   |   0.676470   |     -      |     -     |   8.62   \n","   1    |   280   |   0.553383   |     -      |     -     |   8.72   \n","   1    |   300   |   0.671634   |     -      |     -     |   8.75   \n","   1    |   320   |   0.510207   |     -      |     -     |   8.82   \n","   1    |   340   |   0.538339   |     -      |     -     |   8.82   \n","   1    |   360   |   0.592468   |     -      |     -     |   8.76   \n","   1    |   380   |   0.581936   |     -      |     -     |   8.74   \n","   1    |   400   |   0.612421   |     -      |     -     |   8.70   \n","   1    |   420   |   0.687712   |     -      |     -     |   8.68   \n","   1    |   440   |   0.553090   |     -      |     -     |   8.71   \n","   1    |   460   |   0.517827   |     -      |     -     |   8.73   \n","   1    |   480   |   0.755815   |     -      |     -     |   8.72   \n","   1    |   500   |   0.476911   |     -      |     -     |   8.72   \n","   1    |   520   |   0.547252   |     -      |     -     |   8.73   \n","   1    |   540   |   0.655013   |     -      |     -     |   8.70   \n","   1    |   560   |   0.599668   |     -      |     -     |   8.76   \n","   1    |   580   |   0.529574   |     -      |     -     |   8.73   \n","   1    |   600   |   0.456150   |     -      |     -     |   8.80   \n","   1    |   620   |   0.667225   |     -      |     -     |   8.79   \n","   1    |   640   |   0.616185   |     -      |     -     |   8.73   \n","   1    |   660   |   0.673391   |     -      |     -     |   8.74   \n","   1    |   680   |   0.539279   |     -      |     -     |   8.72   \n","   1    |   700   |   0.573729   |     -      |     -     |   8.73   \n","   1    |   720   |   0.596624   |     -      |     -     |   8.75   \n","   1    |   740   |   0.575393   |     -      |     -     |   8.72   \n","   1    |   760   |   0.485455   |     -      |     -     |   8.70   \n","   1    |   780   |   0.360076   |     -      |     -     |   8.72   \n","   1    |   800   |   0.577819   |     -      |     -     |   8.72   \n","   1    |   820   |   0.510209   |     -      |     -     |   8.75   \n","   1    |   840   |   0.592950   |     -      |     -     |   8.72   \n","   1    |   860   |   0.501667   |     -      |     -     |   8.72   \n","   1    |   880   |   0.355109   |     -      |     -     |   8.67   \n","   1    |   900   |   0.410084   |     -      |     -     |   8.71   \n","   1    |   920   |   0.457094   |     -      |     -     |   8.64   \n","   1    |   940   |   0.521769   |     -      |     -     |   8.70   \n","   1    |   960   |   0.637202   |     -      |     -     |   8.74   \n","   1    |   980   |   0.538645   |     -      |     -     |   8.72   \n","   1    |  1000   |   0.400261   |     -      |     -     |   8.69   \n","   1    |  1020   |   0.544966   |     -      |     -     |   8.71   \n","   1    |  1040   |   0.407539   |     -      |     -     |   8.72   \n","   1    |  1060   |   0.484208   |     -      |     -     |   8.75   \n","   1    |  1080   |   0.512210   |     -      |     -     |   8.64   \n","   1    |  1100   |   0.420983   |     -      |     -     |   8.70   \n","   1    |  1120   |   0.575428   |     -      |     -     |   8.72   \n","   1    |  1140   |   0.471850   |     -      |     -     |   8.72   \n","   1    |  1160   |   0.573297   |     -      |     -     |   8.70   \n","   1    |  1180   |   0.686081   |     -      |     -     |   8.70   \n","   1    |  1200   |   0.321174   |     -      |     -     |   8.65   \n","   1    |  1220   |   0.571586   |     -      |     -     |   8.73   \n","   1    |  1240   |   0.525406   |     -      |     -     |   8.73   \n","   1    |  1249   |   0.497433   |     -      |     -     |   3.81   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   2    |   20    |   0.469152   |     -      |     -     |   9.16   \n","   2    |   40    |   0.476702   |     -      |     -     |   8.74   \n","   2    |   60    |   0.354899   |     -      |     -     |   8.73   \n","   2    |   80    |   0.369931   |     -      |     -     |   8.72   \n","   2    |   100   |   0.364729   |     -      |     -     |   8.73   \n","   2    |   120   |   0.466560   |     -      |     -     |   8.71   \n","   2    |   140   |   0.249668   |     -      |     -     |   8.67   \n","   2    |   160   |   0.503165   |     -      |     -     |   8.70   \n","   2    |   180   |   0.304504   |     -      |     -     |   8.77   \n","   2    |   200   |   0.592836   |     -      |     -     |   8.75   \n","   2    |   220   |   0.452781   |     -      |     -     |   8.70   \n","   2    |   240   |   0.465074   |     -      |     -     |   8.70   \n","   2    |   260   |   0.626273   |     -      |     -     |   8.71   \n","   2    |   280   |   0.443376   |     -      |     -     |   8.69   \n","   2    |   300   |   0.328185   |     -      |     -     |   8.70   \n","   2    |   320   |   0.456592   |     -      |     -     |   8.71   \n","   2    |   340   |   0.544322   |     -      |     -     |   8.70   \n","   2    |   360   |   0.443248   |     -      |     -     |   8.74   \n","   2    |   380   |   0.532695   |     -      |     -     |   8.72   \n","   2    |   400   |   0.399998   |     -      |     -     |   8.74   \n","   2    |   420   |   0.419193   |     -      |     -     |   8.73   \n","   2    |   440   |   0.323107   |     -      |     -     |   8.67   \n","   2    |   460   |   0.446350   |     -      |     -     |   8.74   \n","   2    |   480   |   0.325347   |     -      |     -     |   8.67   \n","   2    |   500   |   0.361035   |     -      |     -     |   8.72   \n","   2    |   520   |   0.262341   |     -      |     -     |   8.64   \n","   2    |   540   |   0.373738   |     -      |     -     |   8.74   \n","   2    |   560   |   0.285760   |     -      |     -     |   8.70   \n","   2    |   580   |   0.454710   |     -      |     -     |   8.68   \n","   2    |   600   |   0.599740   |     -      |     -     |   8.68   \n","   2    |   620   |   0.267312   |     -      |     -     |   8.67   \n","   2    |   640   |   0.300022   |     -      |     -     |   8.66   \n","   2    |   660   |   0.324069   |     -      |     -     |   8.67   \n","   2    |   680   |   0.416748   |     -      |     -     |   8.66   \n","   2    |   700   |   0.383450   |     -      |     -     |   8.71   \n","   2    |   720   |   0.440211   |     -      |     -     |   8.70   \n","   2    |   740   |   0.390676   |     -      |     -     |   8.69   \n","   2    |   760   |   0.505046   |     -      |     -     |   8.69   \n","   2    |   780   |   0.462456   |     -      |     -     |   8.65   \n","   2    |   800   |   0.385292   |     -      |     -     |   8.68   \n","   2    |   820   |   0.537277   |     -      |     -     |   8.70   \n","   2    |   840   |   0.546024   |     -      |     -     |   8.71   \n","   2    |   860   |   0.458827   |     -      |     -     |   8.65   \n","   2    |   880   |   0.439003   |     -      |     -     |   8.77   \n","   2    |   900   |   0.470065   |     -      |     -     |   8.71   \n","   2    |   920   |   0.372367   |     -      |     -     |   8.70   \n","   2    |   940   |   0.390926   |     -      |     -     |   8.71   \n","   2    |   960   |   0.418251   |     -      |     -     |   8.72   \n","   2    |   980   |   0.364399   |     -      |     -     |   8.73   \n","   2    |  1000   |   0.291824   |     -      |     -     |   8.70   \n","   2    |  1020   |   0.367949   |     -      |     -     |   8.68   \n","   2    |  1040   |   0.618694   |     -      |     -     |   8.69   \n","   2    |  1060   |   0.433742   |     -      |     -     |   8.73   \n","   2    |  1080   |   0.350600   |     -      |     -     |   8.76   \n","   2    |  1100   |   0.569634   |     -      |     -     |   8.74   \n","   2    |  1120   |   0.494479   |     -      |     -     |   8.66   \n","   2    |  1140   |   0.394695   |     -      |     -     |   8.68   \n","   2    |  1160   |   0.497388   |     -      |     -     |   8.73   \n","   2    |  1180   |   0.381528   |     -      |     -     |   8.73   \n","   2    |  1200   |   0.426375   |     -      |     -     |   8.70   \n","   2    |  1220   |   0.353415   |     -      |     -     |   8.72   \n","   2    |  1240   |   0.417118   |     -      |     -     |   8.66   \n","   2    |  1249   |   0.306881   |     -      |     -     |   3.83   \n","----------------------------------------------------------------------\n","\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q89oT0n3N0m6"},"source":["## 4. Predictions on Test Set"]},{"cell_type":"markdown","metadata":{"id":"Sqk_CPwjN_W0"},"source":["### 4.1. Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"AzCpJBgWZYR_"},"source":["Before making predictions on the test set, we need to redo processing and encoding steps done on the training data. Fortunately, we have written the `preprocessing_for_bert` function to do that for us."]},{"cell_type":"code","metadata":{"id":"56QTDchdOHBL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619791371412,"user_tz":-120,"elapsed":1209162,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"b4c95eb9-e813-484b-bd2f-4720ac75fb37"},"source":["# Run `preprocessing_for_bert` on the test set\n","print('Tokenizing data...')\n","test_inputs, test_masks = preprocessing_for_bert(imdb_test_data.review)\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=8)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Tokenizing data...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pYv9lSXsQCZ2"},"source":["### 4.2. Predictions"]},{"cell_type":"markdown","metadata":{"id":"YsSlCGCAajmD"},"source":["The threshold we will use is 0.75, meaning that tweets with a predicted probability greater than 75% will be predicted positive. This value is very high compared to the default 0.5 threshold.\n","\n","After manually examining the test set, we can see that the sentiment classification task here is even difficult for human. Therefore, a high threshold will give us safe predictions."]},{"cell_type":"code","metadata":{"id":"aeil24ZjF-pg","executionInfo":{"status":"ok","timestamp":1619791442672,"user_tz":-120,"elapsed":507,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["import torch.nn.functional as F\n","\n","def bert_predict(model, test_dataloader):\n","    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n","    on the test set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    all_logits = []\n","\n","    # For each batch in our test set...\n","    for batch in test_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","        all_logits.append(logits)\n","    \n","    # Concatenate logits from each batch\n","    all_logits = torch.cat(all_logits, dim=0)\n","\n","    # Apply softmax to calculate probabilities\n","    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n","\n","    return probs"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"tGx8h7yXRkfI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619791679495,"user_tz":-120,"elapsed":35331,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"15627cef-ff48-4b87-d54a-79dddc02bed6"},"source":["# Compute predicted probabilities on the test set\n","probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.75\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","# Number of tweets predicted non-negative\n","print(\"Number of reviews predicted non-negative: \", preds.sum())"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Number of reviews predicted non-negative:  461\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"429vKrJ5oHrp","executionInfo":{"status":"ok","timestamp":1619791679496,"user_tz":-120,"elapsed":35099,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}}},"source":["imdb_test_data['preds'] = preds.tolist()\n","imdb_test_data['review'] = imbd_data_complete[5000:5999]['review']\n","imdb_test_data['comparison'] = imdb_test_data.apply(lambda x: x['label'] == str(x['preds']), axis=1)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-gu-sOSpd1V","executionInfo":{"status":"ok","timestamp":1619791679497,"user_tz":-120,"elapsed":34456,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"423d05e3-667d-4574-ee7a-58318b3420dc"},"source":["imdb_test_data.comparison.sum()/imdb_test_data.shape[0]"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8118118118118118"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"tKso6ovmGPWG","executionInfo":{"status":"ok","timestamp":1619791512239,"user_tz":-120,"elapsed":567,"user":{"displayName":"bmon3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjd-4hpliH35AYkl8725DwHeF8OPXIEv3mdwJLawng=s64","userId":"03433765660170101996"}},"outputId":"514c2b50-e780-4c45-d469-7d637f262005"},"source":["imdb_test_data"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>label</th>\n","      <th>id</th>\n","      <th>preds</th>\n","      <th>comparison</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5000</th>\n","      <td>Not a movie for everyone, but this movie is in...</td>\n","      <td>1</td>\n","      <td>5000</td>\n","      <td>1</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5001</th>\n","      <td>This film is not your typical Hollywood fare, ...</td>\n","      <td>1</td>\n","      <td>5001</td>\n","      <td>0</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>5002</th>\n","      <td>RKO Radio Pictures made a real classic in 1947...</td>\n","      <td>1</td>\n","      <td>5002</td>\n","      <td>1</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5003</th>\n","      <td>\"Transylvania 6-5000\" is an insignificant but ...</td>\n","      <td>0</td>\n","      <td>5003</td>\n","      <td>1</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>5004</th>\n","      <td>This is a very good, under-rated action/drama/...</td>\n","      <td>1</td>\n","      <td>5004</td>\n","      <td>1</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5994</th>\n","      <td>I expected a lot more out of this film. The pr...</td>\n","      <td>0</td>\n","      <td>5994</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5995</th>\n","      <td>This was the first movie I ever saw Ashley Jud...</td>\n","      <td>1</td>\n","      <td>5995</td>\n","      <td>1</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5996</th>\n","      <td>I just recently saw this movie in hopes of see...</td>\n","      <td>1</td>\n","      <td>5996</td>\n","      <td>0</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>5997</th>\n","      <td>I remember watching this movie when I was youn...</td>\n","      <td>0</td>\n","      <td>5997</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5998</th>\n","      <td>Annie's wig does not look good. she is not cut...</td>\n","      <td>0</td>\n","      <td>5998</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>999 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                 review  ... comparison\n","5000  Not a movie for everyone, but this movie is in...  ...       True\n","5001  This film is not your typical Hollywood fare, ...  ...      False\n","5002  RKO Radio Pictures made a real classic in 1947...  ...       True\n","5003  \"Transylvania 6-5000\" is an insignificant but ...  ...      False\n","5004  This is a very good, under-rated action/drama/...  ...       True\n","...                                                 ...  ...        ...\n","5994  I expected a lot more out of this film. The pr...  ...       True\n","5995  This was the first movie I ever saw Ashley Jud...  ...       True\n","5996  I just recently saw this movie in hopes of see...  ...      False\n","5997  I remember watching this movie when I was youn...  ...       True\n","5998  Annie's wig does not look good. she is not cut...  ...       True\n","\n","[999 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"k_OcVenocEH_"},"source":["# E - Conclusion"]},{"cell_type":"markdown","metadata":{"id":"UMg9ZUvocF6U"},"source":["Although BERT is very large, complicated, and have millions of parameters, we only need to fine-tune it in only 2 epochs. That result can be achieved because BERT was trained on the huge amount and already encode a lot of information about our language. An impresive performance achieved in a short amount of time, with a small amount of data has shown why BERT is one of the most powerful NLP models available at the moment. "]}]}