<!DOCTYPE html><html><head><meta charset="utf-8"><title>Tecnologías de Procesamiento Big Data.md</title><style></style></head><body id="preview">
<h1 class="code-line" data-line-start=0 data-line-end=1><a id="Tecnologas_de_Procesamiento_Big_Data_0"></a>Tecnologías de Procesamiento Big Data</h1>
<h2 class="code-line" data-line-start=2 data-line-end=3><a id="Enunciado_2"></a>Enunciado</h2>
<p class="has-line-data" data-line-start="4" data-line-end="5">Tras haber realizado la ingesta de datos desde la página de goodreads, ahora tenemos que integrar dentro de nuestro cluster toda la información obtenida.</p>
<p class="has-line-data" data-line-start="6" data-line-end="7">Para ésto lo que vamos a hacer es configurar un <strong>Agente de Flume</strong> cuyo objetivo sea coger los archivos desde un directorio del Edge, añadir los metadatos correspondientes y filtrar la información adquirida, para acabar integrando los datos en el cluster.</p>
<p class="has-line-data" data-line-start="8" data-line-end="9">En concreto, los pasos a realizar serán los siguientes:</p>
<p class="has-line-data" data-line-start="10" data-line-end="11"><strong>SOURCE</strong></p>
<p class="has-line-data" data-line-start="12" data-line-end="13">Coger todos los archivos que estén en un directorio en concreto de nuestro usuario que acaben por ‘.csv’.</p>
<p class="has-line-data" data-line-start="15" data-line-end="16"><strong>INTERCEPTORS</strong></p>
<ol>
<li class="has-line-data" data-line-start="17" data-line-end="18">Añadir una cabecera estática con el origen de nuestros datos (‘goodreads’).</li>
<li class="has-line-data" data-line-start="18" data-line-end="19">Eliminar todas las líneas que tengan la palabra ‘ebook’, dado que no es un género real y no nos interesa para los análisis posteriores.</li>
<li class="has-line-data" data-line-start="19" data-line-end="21">Extraer el campo de ‘genre’ de cada línea y añadirla como metadatos de la cabecera para el posterior particionamiento de la tabla en Hive.</li>
</ol>
<p class="has-line-data" data-line-start="21" data-line-end="22"><strong>REPLICATING</strong></p>
<p class="has-line-data" data-line-start="23" data-line-end="24">Habrá que configurar un Selector que envíe los datos a dos fuentes distintas, según la utilidad que se le vaya a dar a los datos.</p>
<p class="has-line-data" data-line-start="25" data-line-end="26"><strong>CHANNELS</strong></p>
<p class="has-line-data" data-line-start="27" data-line-end="28">Elección libre</p>
<p class="has-line-data" data-line-start="29" data-line-end="30"><strong>SINKS</strong></p>
<ol>
<li class="has-line-data" data-line-start="30" data-line-end="31">HDFS (en el directorio ‘./flume_data/CABECERA_ESTÁTICA’)</li>
<li class="has-line-data" data-line-start="31" data-line-end="33">Hive (a una tabla llamada ‘goodreads_flume’)</li>
</ol>
<p class="has-line-data" data-line-start="33" data-line-end="34">De forma que el agente de Flume seguirá un esquema como el siguiente:</p>
<p class="has-line-data" data-line-start="35" data-line-end="36"><img src="esquema.png" alt="esquema"></p>
<hr>
<h2 class="code-line" data-line-start=39 data-line-end=40><a id="Agente_39"></a>Agente</h2>
<blockquote>
<p class="has-line-data" data-line-start="41" data-line-end="42">#FLUME AGENT - Practica FLUME - Beltran Rodriguez-Mon</p>
<p class="has-line-data" data-line-start="43" data-line-end="44">##Flume agent elements</p>
<p class="has-line-data" data-line-start="45" data-line-end="48">brodriguez.sources = spoolsource<br>
brodriguez.channels = channel1 channel2<br>
brodriguez.sinks = hdfs_sink hive_sink</p>
<p class="has-line-data" data-line-start="49" data-line-end="50">##Source configuration</p>
<p class="has-line-data" data-line-start="51" data-line-end="54">brodriguez.sources.spoolsource.type = spooldir<br>
brodriguez.sources.spoolsource.spoolDir = /home/alumnos18/brodriguez/input<br>
brodriguez.sources.spoolsource.includePattern = .+(.csv)</p>
<p class="has-line-data" data-line-start="55" data-line-end="56">##Interceptors</p>
<p class="has-line-data" data-line-start="57" data-line-end="58">brodriguez.sources.spoolsource.interceptors = staticInterceptor filterRegexInterceptor extractorRegexInterceptor</p>
<p class="has-line-data" data-line-start="59" data-line-end="60">##Static Interceptor configuration</p>
<p class="has-line-data" data-line-start="61" data-line-end="64">brodriguez.sources.spoolsource.interceptors.staticInterceptor.type = static<br>
brodriguez.sources.spoolsource.interceptors.staticInterceptor.key = source<br>
brodriguez.sources.spoolsource.interceptors.staticInterceptor.value = GoodReads</p>
<p class="has-line-data" data-line-start="65" data-line-end="66">##Regex Filter Interceptor configuration</p>
<p class="has-line-data" data-line-start="67" data-line-end="69">brodriguez.sources.spoolsource.interceptors.filterRegexInterceptor.type = regex_filter<br>
brodriguez.sources.spoolsource.interceptors.filterRegexInterceptor.regex = ^((?!ebooks).)*$</p>
<p class="has-line-data" data-line-start="70" data-line-end="71">##RegexExtractor Interceptor configuration</p>
<p class="has-line-data" data-line-start="72" data-line-end="76">brodriguez.sources.spoolsource.interceptors.extractorRegexInterceptor.type = regex_extractor<br>
brodriguez.sources.spoolsource.interceptors.extractorRegexInterceptor.regex = FALTA_REGEX_GENRE<br>
brodriguez.sources.spoolsource.interceptors.extractorRegexInterceptor.serializers = s1<br>
<a href="http://brodriguez.sources.spoolsource.interceptors.extractorRegexInterceptor.serializers.s1.name">brodriguez.sources.spoolsource.interceptors.extractorRegexInterceptor.serializers.s1.name</a> = g</p>
<p class="has-line-data" data-line-start="77" data-line-end="78">##Replicator configuration</p>
<p class="has-line-data" data-line-start="79" data-line-end="81">brodriguez.sources.spoolsource.selector.type = replicating<br>
brodriguez.sources.spoolsource.channels = channel1 channel2</p>
<p class="has-line-data" data-line-start="82" data-line-end="83">##Channel1 configuration</p>
<p class="has-line-data" data-line-start="84" data-line-end="87">brodriguez.channels.channel1.type = memory<br>
brodriguez.channels.channel1.capacity = 100000<br>
brodriguez.channels.channel1.transactionCapacity = 1000</p>
<p class="has-line-data" data-line-start="88" data-line-end="89">##Channel2 configuration</p>
<p class="has-line-data" data-line-start="90" data-line-end="93">brodriguez.channels.channel2.type = memory<br>
brodriguez.channels.channel2.capacity = 100000<br>
brodriguez.channels.channel2.transactionCapacity = 1000</p>
<p class="has-line-data" data-line-start="94" data-line-end="95">##HDFS Sink configuration</p>
<p class="has-line-data" data-line-start="96" data-line-end="105">brodriguez.sinks.hdfs_sink.type = hdfs<br>
brodriguez.sinks.hdfs_sink.hdfs.path = hdfs://nameservice1/user/brodriguez/flume_data/%{filetype}<br>
brodriguez.sinks.hdfs_sink.hdfs.fileType = DataStream<br>
brodriguez.sinks.hdfs_sink.hdfs.writeFormat = Text<br>
brodriguez.sinks.hdfs_sink.hdfs.batchSize = 1000<br>
brodriguez.sinks.hdfs_sink.hdfs.rollSize = 0<br>
brodriguez.sinks.hdfs_sink.hdfs.rollCount = 1000<br>
brodriguez.sinks.hdfs_sink.hdfs.rollInterval = 50<br>
brodriguez.sinks.hdfs_sink.hdfs.useLocalTimeStamp = true</p>
<p class="has-line-data" data-line-start="106" data-line-end="107">##Hive Sink configuration</p>
<p class="has-line-data" data-line-start="108" data-line-end="116">brodriguez.sinks.hive_sink.type=hive<br>
brodriguez.sinks.hive_sink.hive.metastore=thrift://master02.bigdata.alumnos.upcont.es:9083<br>
brodriguez.sinks.hive_sink.hive.database=brodriguez<br>
brodriguez.sinks.hive_sink.hive.table=goodreads_flume<br>
brodriguez.sinks.hive_sink.hive.partition=%{g}<br>
brodriguez.sinks.hive_sink.serializer=DELIMITED<br>
brodriguez.sinks.hive_sink.serializer.delimiter=;<br>
brodriguez.sinks.hive_sink.serializer.fieldnames=title,author,description,rating,ratingCount,reviewCount,genre,book_link</p>
<p class="has-line-data" data-line-start="117" data-line-end="118">##Channels binding</p>
<p class="has-line-data" data-line-start="119" data-line-end="122">brodriguez.sources.spoolsource.channels = channel1 channel2<br>
brodriguez.sinks.hdfs_sink.channel = channel1<br>
brodriguez.sinks.hive_sink.channel = channel2</p>
</blockquote>
<hr>
<h2 class="code-line" data-line-start=125 data-line-end=126><a id="Tabla_Hive_125"></a>Tabla Hive</h2>
<blockquote>
<p class="has-line-data" data-line-start="127" data-line-end="139">CREATE TABLE brodriguez.goodreads_flume<br>
(<br>
title STRING,<br>
author STRING,<br>
description STRING,<br>
rating INT,<br>
ratingCount INT,<br>
reviewCount INT,<br>
book_link STRING,<br>
)<br>
PARTITIONED BY (g STRING)<br>
STORED AS ORC;</p>
</blockquote>
</body></html>